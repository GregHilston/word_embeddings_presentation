{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following may need to be run inside your terminal if linking fails"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!/Users/ghilston/.virtualenvs/word_embeddings/bin/python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the word embeddings that we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim with Glove vectors, \n",
    "glove_vectors = KeyedVectors.load_word2vec_format('./vectors/glove_840B/word2vec.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Words With SpacY _(Done By Hand)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets demonstrate the power of word embeddings by generating similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_similar_words(target, n=10, reverse=False): \n",
    "    if isinstance(target, str):\n",
    "        target = nlp.vocab[target].vector # the vector of our target word\n",
    "\n",
    "    top_n_similar_words = [] # the list of similar words we'll be returning\n",
    "    \n",
    "    cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y) # our helper function for calculating similarity between two vectors\n",
    "\n",
    "    computed_similarities = [] # a list of vectors for all possible words\n",
    "    \n",
    "    for word in nlp.vocab:\n",
    "        # Ignore words without vectors\n",
    "        if not word.has_vector:\n",
    "            continue\n",
    "\n",
    "        similarity = cosine_similarity(target, word.vector)\n",
    "        computed_similarities.append((word, similarity))\n",
    "\n",
    "    if reverse:\n",
    "        computed_similarities = sorted(computed_similarities, key=lambda item: -item[1], reverse=True) # computing our similarities\n",
    "    else: \n",
    "        computed_similarities = sorted(computed_similarities, key=lambda item: -item[1]) # computing our similarities\n",
    "    \n",
    "    similar_words = [w[0].text for w in computed_similarities] # grabbing the text component of our similiar vectors\n",
    "    \n",
    "    index = 0\n",
    "    while(len(top_n_similar_words) != n):\n",
    "        word = similar_words[index].lower() # making words case insensitive to reduce duplicates\n",
    "        # print(index)\n",
    "        # print(f\"evaluating {word}\")\n",
    "        \n",
    "        if word not in top_n_similar_words: # duplicate check\n",
    "            top_n_similar_words.append(word)\n",
    "            # print(f\"adding {word}\")\n",
    "            # print(f\"top_n_similar_words {top_n_similar_words}\\n\")\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    return top_n_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kumara',\n",
       " 'tuber',\n",
       " 'sweet-potato',\n",
       " 'chickpea',\n",
       " 'potato',\n",
       " 'potatoe',\n",
       " 'yuca',\n",
       " 'tuberosum',\n",
       " 'spud',\n",
       " 'taro']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_similar_words(\"potato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Words with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['potatoes',\n",
       " 'carrot',\n",
       " 'tomato',\n",
       " 'onion',\n",
       " 'salad',\n",
       " 'mashed',\n",
       " 'cabbage',\n",
       " 'spinach',\n",
       " 'rice',\n",
       " 'carrots']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = [i[0] for i in glove_vectors.most_similar(positive=[\"potato\"], topn=10)]\n",
    "glove_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Built In Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12974958259688887324], dtype=uint64),\n",
       " array([3596], dtype=int32),\n",
       " array([1.], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 = nlp(u\"potato\").vector\n",
    "most_similar = nlp(u\"potato\").vocab.vectors.most_similar(vector1.reshape(1, vector1.shape[0]))\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet-potato'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[12974958259688887324].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opposite (Least Similar) Word\n",
    "\n",
    "With a simple reverse of the sorted similarity scores we can come up with the least similar words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mason-dixon',\n",
       " '85-plus',\n",
       " 'swd',\n",
       " 'srd',\n",
       " 'wotc',\n",
       " 'coys',\n",
       " 'stainers',\n",
       " 'cargo-handling',\n",
       " 'superpremium',\n",
       " '21-jewel']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_similar_words(\"potato\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Arithmatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets show how we can do simple arithmatic with word embeddings. \n",
    "\n",
    "A king and queen are the male and female version of the same royal position. Lets see if our word embeddings can prove to us it knows this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'queen',\n",
       " 'prince',\n",
       " 'commoner',\n",
       " 'highness',\n",
       " 'sultan',\n",
       " 'maharajas',\n",
       " 'kings',\n",
       " 'princes',\n",
       " 'sultans']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "king = nlp.vocab[\"king\"].vector\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    " \n",
    "maybe_queen = king - man + woman\n",
    "\n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "top_n_similar_words(maybe_queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Arithmatic with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.775162398815155),\n",
       " ('prince', 0.6123066544532776),\n",
       " ('princess', 0.6016970872879028),\n",
       " ('kings', 0.5996100902557373),\n",
       " ('queens', 0.565579891204834),\n",
       " ('royal', 0.5646308660507202),\n",
       " ('throne', 0.5580971240997314),\n",
       " ('Queen', 0.5569202899932861),\n",
       " ('monarch', 0.5499411821365356),\n",
       " ('empress', 0.5295248627662659)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Lets say we wanted to get similar words for billiards, aka pool tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swimmingpool',\n",
       " 'swimming-pool',\n",
       " 'sauna',\n",
       " 'pool',\n",
       " 'chlorinators',\n",
       " 'swim-up',\n",
       " 'pools',\n",
       " 'inground',\n",
       " 'cabanas',\n",
       " 'hottubs']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_similar_words(\"pool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that all of our words are coming back as swimming pools. What can we do?\n",
    "\n",
    "We could:\n",
    "\n",
    "- manually prune this list\n",
    "- do string matching\n",
    "- hire an intern (free labor)\n",
    "\n",
    "Instead, lets use word embedddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com-plete',\n",
       " 'synbiotics',\n",
       " 'emazing',\n",
       " 'selectives',\n",
       " '451.7',\n",
       " '120-million',\n",
       " 'lenni',\n",
       " '518.6',\n",
       " '37lbs',\n",
       " 'pilons']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "pool = nlp.vocab[\"pool\"].vector\n",
    "swimming = nlp.vocab[\"swimming\"].vector\n",
    "water = nlp.vocab[\"water\"].vector\n",
    " \n",
    "billiards = (pool - swimming) - water\n",
    "\n",
    "top_n_similar_words(billiards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pool', 0.4190593957901001),\n",
       " ('Poolside', 0.4075739085674286),\n",
       " ('jacuzzi', 0.40432262420654297),\n",
       " ('billiards', 0.3948186933994293),\n",
       " ('Jacuzzi', 0.38681161403656006),\n",
       " ('Penthouse', 0.37995702028274536),\n",
       " ('poolside', 0.37497371435165405),\n",
       " ('solarium', 0.3684753179550171),\n",
       " ('Billiards', 0.36510801315307617),\n",
       " ('Clubhouse', 0.36037442088127136)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(positive=['pool'], negative=['water'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AfricaLinking', 0.36948105692863464),\n",
       " ('TrainingResources', 0.33629927039146423),\n",
       " ('DrivenPrice', 0.33019590377807617),\n",
       " ('6/Pkg', 0.32633116841316223),\n",
       " ('MzAwNTo1OjEw', 0.32195737957954407),\n",
       " ('ShowId', 0.32156652212142944),\n",
       " ('Footnication', 0.3199766278266907),\n",
       " ('VERNIS', 0.3184490501880646),\n",
       " ('ThermalPrice', 0.3171261250972748),\n",
       " ('TTC-After-Loss', 0.31558042764663696)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(positive=['pool'], negative=['water', 'swimming'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word_embeddings",
   "language": "python",
   "name": "word_embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
