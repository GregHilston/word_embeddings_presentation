{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Workbook\n",
    "\n",
    "This notebook was designed to be able to be worked through from start to finish by anyone with prior Python experience. We'll be exploring Word Embeddings and how to use them in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll be using two libraries today: \n",
    "\n",
    "- spacy\n",
    "  - contains the ability to work with vectors, cosine similarity, vocabulary and more\n",
    "  - generally seen as easy to use\n",
    "- gensim\n",
    "  - contains the ability to work with vectors, cosine similarity and more\n",
    "  - generallys een as harder to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the libraries loaded, we're going to leverage pre existing word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim with Glove vectors, \n",
    "word2vec = KeyedVectors.load_word2vec_format('./vectors/glove_6B/word2vec.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the word2vec format returns us an object of type `Word2VecKeyedVectors`. Take a moment and scroll through the documentation for this class, located [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Words\n",
    "\n",
    "At this point we're ready to ask ask our glove vectors for similar words. To do this we'll be using the `most_similar` function you hopefully just discovered. \n",
    "\n",
    "This function takes in a list of positive words, a list of negative words and how many from the top (most relevant) it should return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.most_similar(positive=[\"king\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note at the second value that comes back. This is a rating from $0$ to $1.0$, ranking how relevant it believes that particular word is.\n",
    "\n",
    "Lets try to to use the negative list now. Perhaps we wanted to get royal positions that were for women only. We can use the same function and pass in negative values now. Let's demonstrate that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be all related royal positions that have no \"maleness\". This is because we subtracted the vector of the value `\"man\"`. This is an incredibly powerful and useful tool to use. \n",
    "\n",
    "Let's have you try out your own `most_similar` calls now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_of_choice = \"\" # replace the empty string with your word of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.most_similar(positive=[word_of_choice], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were the results relevant? Feel free to experiment with a few different words. \n",
    "\n",
    "If you found anything particularly interesting, please share with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "### Preparing Our Neural Network\n",
    "\n",
    "So at this point we're going to demonstrate sentiment analysis. The next couple of cells are required to be ran without modification."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"sentiment_analysis_training.txt\", sep=\"\\t\", names=[\"sentiment\", \"text\"])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "documents = df[\"text\"].tolist()\n",
    "labels = np.array(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to present our documents, which are all reviews for a local pizza place. Along with this we're classifying the reviews, $1$ for good and $0$ for bad. Which we're manually labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"oh my god! So good. Philly grinder!\",\n",
    "             \"This is one of the best pizza's in town. The 7 cheese pizza is Amazing.... stuffed churros, yum. Great upbeat staff. I could eat here every night.\",\n",
    "             \"I'm telling you YOUR SERVICE STINKS AND YOU'D RETAIN MORE CUSTOMERS IF YOU'D JUST TEACH THEM TO USE THE PHONE PROPERLY AND TAKE AN ORDER PROPERLY.\",\n",
    "             \"My favorite Fort Collins pizza place! I always go for the south of the border. They also have the best spicy ranch.\",\n",
    "             \"Nope. Don't do it. Ordered a pizza over 2 hours ago and it still hasn't shown up.\",\n",
    "             \"My favorite pizza place in fort collins! I always order South of the border with cream cheese the staff are awesome. One time the pizza deliver guy came super late. He apologized and gave us restaurant credit.\"]\n",
    "\n",
    "labels = np.array([1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to process the documents:\n",
    "\n",
    "- stripping the punctuation\n",
    "- encoding the documents as vectors from our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_documents = []\n",
    "for line in documents:\n",
    "    line = strip_punctuation(line)\n",
    "    encoded = []\n",
    "    for word in line.split():\n",
    "        try:\n",
    "            encoded.append(word2vec.vocab[word].index)\n",
    "        except:\n",
    "            encoded.append(0)\n",
    "    encoded_documents.append(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network, all of our input needs to be the same length, so we'll be padding $0$s at the end to make them the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(i.split()) for i in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_documents = pad_sequences(encoded_documents, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_input_dim = word2vec[word2vec.vocab].shape[0]\n",
    "embedding_output_dim = word2vec[word2vec.vocab].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll be building our neural network using Keras, which makes this incredilby easy.\n",
    "\n",
    "We'll be feeding in our padded doucments for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_input_dim, output_dim=embedding_output_dim, weights=[word2vec[word2vec.vocab]], input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "model.fit(padded_documents, labels, epochs=50, verbose=0)\n",
    "model.evaluate(padded_documents, labels, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Our Neural Network To Do Predictions\n",
    "\n",
    "At this point we're ready to test our neural network. Don't edit these cells at this moment, you'll be asked to go back and edit the `documents` variable later.\n",
    "\n",
    "This variable, `documents`, represents reviews our neural network hasn't seen before. We're going to ask it if these sentences are good, represented by a $1$, or bad, represented by a $0$.\n",
    "\n",
    "Note, the first document is positive and the second is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"best pizza in fort collins\",\n",
    "             \"The pizza is bad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we're going to process the documents:\n",
    "\n",
    "- stripping the punctuation\n",
    "- encoding the documents as vectors from our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_documents = []\n",
    "for line in documents:\n",
    "    line = strip_punctuation(line)\n",
    "    encoded = []\n",
    "    for word in line.split():\n",
    "        try:\n",
    "            encoded.append(word2vec.vocab[word].index)\n",
    "        except:\n",
    "            encoded.append(0)\n",
    "    encoded_documents.append(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we're going to pad our sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_documents = pad_sequences(encoded_documents, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask the neural network to predict the sentiment of the sentences. We expect the first one to be closer to one, which representes better, and the second one to be closer to zero, which represents worse.\n",
    "\n",
    "Better and worse meaning positive and negative respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(padded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(padded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can have it round to the closest $0$, or $1$.\n",
    "\n",
    "Feel free to go up above and change the `documents` variable and rerun the code cells to see how it performs. Your changes should be based on pizza and won't work to well, as we had a relatively small amount of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word_embeddings",
   "language": "python",
   "name": "word_embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
